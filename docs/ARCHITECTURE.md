# ğŸ—ï¸ Arquitetura TÃ©cnica - Sage's Oracle

## VisÃ£o Geral

Sage's Oracle Ã© um assistente de IA para D&D 5e que utiliza a arquitetura **RAG (Retrieval-Augmented Generation)** para fornecer respostas precisas e fundamentadas sobre regras, magias e monstros.

## Diagrama de Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Streamlit)                  â”‚
â”‚  - Interface de chat                                     â”‚
â”‚  - ExibiÃ§Ã£o de fontes                                   â”‚
â”‚  - ConfiguraÃ§Ãµes (top_k, temperature)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTP/REST
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API BACKEND (FastAPI)                       â”‚
â”‚  Endpoints:                                              â”‚
â”‚  - POST /ask         â†’ Faz pergunta                     â”‚
â”‚  - GET  /health      â†’ Status da API                    â”‚
â”‚  - GET  /sources/:type â†’ Lista fontes                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG ENGINE (Core)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Query Embedding                               â”‚  â”‚
â”‚  â”‚     - Transforma pergunta em vetor                â”‚  â”‚
â”‚  â”‚                                                   â”‚  â”‚
â”‚  â”‚  2. Retrieval (Busca SemÃ¢ntica)                   â”‚  â”‚
â”‚  â”‚     - Similaridade coseno                         â”‚  â”‚
â”‚  â”‚     - Top-K chunks mais relevantes                â”‚  â”‚
â”‚  â”‚                                                   â”‚  â”‚
â”‚  â”‚  3. Context Building                              â”‚  â”‚
â”‚  â”‚     - Monta prompt com documentos recuperados     â”‚  â”‚
â”‚  â”‚                                                   â”‚  â”‚
â”‚  â”‚  4. LLM Generation                                â”‚  â”‚
â”‚  â”‚     - Chama Ollama (Phi-3 Mini)                   â”‚  â”‚
â”‚  â”‚     - Prompt engineering com guardrails           â”‚  â”‚
â”‚  â”‚                                                   â”‚  â”‚
â”‚  â”‚  5. Response + Source Citation                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embeddings â”‚      â”‚    Chunks    â”‚
â”‚   (.npy)    â”‚      â”‚    (.json)   â”‚
â”‚             â”‚      â”‚              â”‚
â”‚ - 384 dims  â”‚      â”‚ - Text       â”‚
â”‚ - NumPy     â”‚      â”‚ - Metadata   â”‚
â”‚             â”‚      â”‚ - Tokens     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                     â–²
      â”‚                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ETL Pipeline   â”‚
    â”‚                  â”‚
    â”‚ 1. Scraper       â”‚
    â”‚ 2. Chunker       â”‚
    â”‚ 3. Embedder      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–²
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Data Sources    â”‚
    â”‚                  â”‚
    â”‚ - D&D 5e API     â”‚
    â”‚ - SRD Content    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Stack TecnolÃ³gica

### Backend
- **FastAPI**: Framework web moderno, async, com validaÃ§Ã£o automÃ¡tica
- **Pydantic**: ValidaÃ§Ã£o de dados e modelos
- **Uvicorn**: ASGI server

### IA & ML
- **Sentence Transformers**: GeraÃ§Ã£o de embeddings semÃ¢nticos
  - Modelo: `all-MiniLM-L6-v2` (384 dimensÃµes, 80MB)
- **Ollama**: Servidor local de LLMs
  - Modelo: `phi3:mini` (3.8B parÃ¢metros, ~2.3GB)
- **NumPy**: OperaÃ§Ãµes vetoriais e similaridade coseno

### ETL
- **Requests**: HTTP client para scraping de APIs
- **tqdm**: Progress bars
- **tiktoken**: TokenizaÃ§Ã£o para contagem

### Frontend
- **Streamlit**: Framework para apps de dados/ML

## DecisÃµes Arquiteturais

### 1. Por que Embeddings Locais?

**DecisÃ£o**: Usar `all-MiniLM-L6-v2` localmente ao invÃ©s de APIs externas (OpenAI, Cohere).

**Motivo**:
- âœ… Sem custo de API
- âœ… Sem latÃªncia de rede
- âœ… Privacidade total
- âœ… Funciona offline
- âš ï¸ Trade-off: Qualidade um pouco menor que modelos gigantes

### 2. Por que Phi-3 Mini?

**DecisÃ£o**: Usar Phi-3 Mini (3.8B) via Ollama ao invÃ©s de GPT-4 ou modelos maiores.

**Motivo**:
- âœ… Roda em hardware modesto (RTX 3050, 8GB VRAM)
- âœ… RÃ¡pido para inferÃªncia
- âœ… Qualidade excelente para o tamanho
- âœ… Sem custos de API
- âš ï¸ Trade-off: Menos "criativo" que modelos de 70B+, mas perfeito para Q&A factual

### 3. EstratÃ©gia de Chunking

**DecisÃ£o**: Chunking diferenciado por tipo de conteÃºdo.

| Tipo | EstratÃ©gia | Max Tokens | Overlap | Motivo |
|------|-----------|-----------|---------|--------|
| **Spells** | Entity-based | 300 | 0 | Magias sÃ£o auto-contidas |
| **Monsters** | Entity-based | 400 | 0 | Stat blocks completos |
| **Rules** | Section-based | 512 | 50 | Regras precisam de contexto |

**Por quÃª?**
- Chunks muito pequenos perdem contexto
- Chunks muito grandes prejudicam a precisÃ£o do retrieval
- Overlap em regras evita cortar conceitos ao meio

### 4. Busca Puramente Vetorial (vs. HÃ­brida)

**DecisÃ£o Atual**: Busca vetorial com similaridade coseno.

**PossÃ­vel Melhoria Futura**: Implementar busca hÃ­brida (vetorial + BM25).

**Trade-off**:
- âœ… Simples de implementar
- âœ… Funciona bem para perguntas semÃ¢nticas
- âš ï¸ Pode falhar em buscas por nome exato (ex: "Fireball" vs "Fire Ball")

### 5. Prompt Engineering

**DecisÃ£o**: Prompt com guardrails explÃ­citos.

```python
Guardrails implementados:
1. "Use ONLY the provided documents"
2. "If not in documents, say you don't know"
3. "Cite document numbers"
4. "Be concise but complete"
```

**Por quÃª?**
- Previne alucinaÃ§Ãµes
- ForÃ§a rastreabilidade (citaÃ§Ãµes)
- MantÃ©m respostas fundamentadas

## MÃ©tricas de Performance

### Tamanho dos Dados

| Componente | Tamanho | Quantidade |
|-----------|---------|-----------|
| Raw JSON | ~5MB | 5 arquivos |
| Processed Chunks | ~3MB | 600-800 chunks |
| Embeddings | ~2MB | 600-800 Ã— 384 dims |
| **Total** | **~10MB** | - |

### Tempo de Processamento (ETL)

| Etapa | Tempo (estimado) |
|-------|-----------------|
| Scraping | ~2-3 minutos |
| Chunking | ~10 segundos |
| Embeddings | ~30 segundos |
| **Total** | **~3-4 minutos** |

### LatÃªncia (Query)

| Etapa | Tempo |
|-------|-------|
| Embedding da query | ~50ms |
| Retrieval (top-5) | ~20ms |
| LLM Generation | ~2-5s (dependendo do comprimento) |
| **Total** | **~2-5s** |

## Escalabilidade

### LimitaÃ§Ãµes Atuais

- **Busca Linear**: O(n) para busca em ~800 chunks (aceitÃ¡vel para essa escala)
- **Single-threaded**: Sem paralelizaÃ§Ã£o
- **In-memory**: Tudo em RAM

### Como Escalar (Futuro)

1. **Para 10K+ chunks**:
   - Migrar para FAISS com Ã­ndice IVF
   - Ou usar Milvus/Qdrant

2. **Para mÃºltiplos usuÃ¡rios**:
   - Adicionar Redis para cache
   - Load balancer para mÃºltiplas instÃ¢ncias da API

3. **Para LLMs maiores**:
   - Usar GPU remota (RunPod, Together.ai)
   - Ou quantizaÃ§Ã£o INT8/INT4

## SeguranÃ§a

### Implementado

- âœ… CORS configurado
- âœ… Guardrails no prompt (anti-alucinaÃ§Ã£o)
- âœ… Timeout nas requisiÃ§Ãµes

### A Implementar (ProduÃ§Ã£o)

- [ ] Rate limiting
- [ ] API Keys
- [ ] Input sanitization
- [ ] Logging de queries

## Testes

### Casos de Teste Recomendados

1. **Retrieval Accuracy**
   - Query: "Fireball"
   - Esperado: Chunk da magia Fireball no top-1

2. **Out-of-scope**
   - Query: "How to cook pasta?"
   - Esperado: "I don't have that information..."

3. **Multi-hop**
   - Query: "What spells can a 5th level wizard cast?"
   - Esperado: Combinar info de wizard + spell slots

## ReferÃªncias

- [D&D 5e API](https://www.dnd5eapi.co/)
- [Sentence Transformers](https://www.sbert.net/)
- [Ollama](https://ollama.ai/)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/)

